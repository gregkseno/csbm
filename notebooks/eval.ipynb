{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "729e75c8-837b-463b-b4e6-d3fc71f4b7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys \n",
    "sys.path.append('scripts')\n",
    "sys.path.append('src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f4a406-3e81-4519-8325-dd89ee4e77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_fid.fid_score import calculate_fid_given_paths\n",
    "\n",
    "from csbm.data import BaseDataset\n",
    "from csbm.metrics import CMMD, FID, MSE\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d036d2f7",
   "metadata": {},
   "source": [
    "## FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bc7f8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SomeDataset(BaseDataset):\n",
    "    transform: Optional[transforms.Compose] = None\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        data_dir: str,\n",
    "    ):\n",
    "        self.data_dir= data_dir\n",
    "\n",
    "        self.dataset = os.listdir(data_dir)\n",
    "        self.dataset = [os.path.join(data_dir, x) for x in self.dataset]\n",
    "        self.dataset = list(filter(lambda x: x.endswith('.jpg'), self.dataset))\n",
    "        self.dataset = sorted(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        transform = transforms.ToTensor()\n",
    "        image = Image.open(self.dataset[index])\n",
    "        image = image.convert('RGB')\n",
    "        image = transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b385277-c54c-44a1-ba98-2e24bfe97eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 370/370 [00:26<00:00, 14.17it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 506/506 [00:35<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4, FID: 9.916346980044523\n"
     ]
    }
   ],
   "source": [
    "iteration = 4\n",
    "ref_data_path = 'data/celeba/female_test'\n",
    "\n",
    "for exp_name in ['dim_128_aplha_0.005_27.01.25_21:56:36']:# , 'dim_128_aplha_0.01_14.01.25_21:22:30']:\n",
    "    gen_data_path = f'experiments/quantized_images/celeba/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "    cmmd = calculate_fid_given_paths(\n",
    "        paths=[ref_data_path, gen_data_path],\n",
    "        dims=2048,\n",
    "        batch_size=32,\n",
    "        device=device,\n",
    "    )\n",
    "    print(f'Iter: {iteration}, FID: {cmmd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8580d5-bb77-4640-8ed5-e71cec460ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/trinity/home/g.ksenofontov/anaconda3/envs/csbm/lib/python3.12/site-packages/torch_fidelity/feature_extractor_inceptionv3.py:85: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(feature_extractor_weights_path)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e7ee9700454b7ebc08eaf8ceeacd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03527d55304644d3b33a516fd0bfde82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4, FID: 9.969728469848633\n"
     ]
    }
   ],
   "source": [
    "iteration = 4\n",
    "ref_data_path = 'data/celeba/female_test'\n",
    "\n",
    "for exp_name in ['dim_128_aplha_0.005_27.01.25_21:56:36']:#, 'dim_128_aplha_0.01_14.01.25_21:22:30']: \n",
    "    cmmd = FID().to(device)\n",
    "    gen_data_path = f'experiments/quantized_images/celeba/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        SomeDataset(data_dir=ref_data_path), batch_size=32\n",
    "    )\n",
    "    for real_images in tqdm(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        cmmd.update(real_images, real=True)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        SomeDataset(data_dir=gen_data_path), batch_size=32\n",
    "    )\n",
    "    for fake_images in tqdm(dataloader):\n",
    "        fake_images = fake_images.to(device)\n",
    "        cmmd.update(fake_images, real=False)\n",
    "    \n",
    "    print(f'Iter: {iteration}, FID: {cmmd.compute().detach().cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e4bfd6",
   "metadata": {},
   "source": [
    "## CMMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa120680",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(embd_ref_path):\n",
    "    embs_ref = np.load(embd_ref_path).astype(\"float32\")\n",
    "else:\n",
    "    embs_ref = compute_embeddings_for_dir(\n",
    "        os.path.join(ref_data_path),\n",
    "        ClipEmbeddingModel(), batch_size, max_count\n",
    "    ).astype(\"float32\")\n",
    "    np.save(embd_ref_path, embs_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e876bd76-95c9-425c-ab8f-24770486d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp_name in ['dim_128_aplha_0.005_27.01.25_21:56:36']: # ['dim_128_aplha_0.01_14.01.25_21:22:30']: #, 'small_dim_128_aplha_0.01_20.01.25_16:43:26']: #'tiny_dim_128_aplha_0.01_17.01.25_22:02:58', 'tiny_dim_128_aplha_0.01_19.01.25_21:21:21']:\n",
    "    for iteration in range(4, 5):\n",
    "        gen_data_path = f'../experiments/quantized_images/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "        cmmd = calculate_cmmd(\n",
    "            eval_dir=gen_data_path,\n",
    "            embs_ref=embs_ref,\n",
    "            batch_size=batch_size,\n",
    "            max_count=max_count\n",
    "        )\n",
    "        print(f'CMMD: {cmmd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bdf5314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cc6b59c0ff745d7916e8c263da47279",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e0f911c6064ef1bbdc5d13e43da5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected np.ndarray (got Tensor)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m     fake_images = fake_images.to(device)\n\u001b[32m     21\u001b[39m     fid.update(fake_images, real=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, CMMD: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mfid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.detach().cpu().numpy()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/csbm/lib/python3.12/site-packages/torchmetrics/metric.py:699\u001b[39m, in \u001b[36mMetric._wrap_compute.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;66;03m# compute relies on the sync context manager to gather the states across processes and apply reduction\u001b[39;00m\n\u001b[32m    692\u001b[39m \u001b[38;5;66;03m# if synchronization happened, the current rank accumulated states will be restored to keep\u001b[39;00m\n\u001b[32m    693\u001b[39m \u001b[38;5;66;03m# accumulation going if ``should_unsync=True``,\u001b[39;00m\n\u001b[32m    694\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sync_context(\n\u001b[32m    695\u001b[39m     dist_sync_fn=\u001b[38;5;28mself\u001b[39m.dist_sync_fn,\n\u001b[32m    696\u001b[39m     should_sync=\u001b[38;5;28mself\u001b[39m._to_sync,\n\u001b[32m    697\u001b[39m     should_unsync=\u001b[38;5;28mself\u001b[39m._should_unsync,\n\u001b[32m    698\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m     value = _squeeze_if_scalar(\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    700\u001b[39m     \u001b[38;5;66;03m# clone tensor to avoid in-place operations after compute, altering already computed results\u001b[39;00m\n\u001b[32m    701\u001b[39m     value = apply_to_collection(value, Tensor, \u001b[38;5;28;01mlambda\u001b[39;00m x: x.clone())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/beegfs/home/g.ksenofontov/Projects/csbm/src/csbm/metrics.py:132\u001b[39m, in \u001b[36mcompute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      0\u001b[39m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/beegfs/home/g.ksenofontov/Projects/csbm/src/csbm/metrics.py:86\u001b[39m, in \u001b[36m_mmd\u001b[39m\u001b[34m(self, x, y, sigma, scale)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_mmd\u001b[39m(\n\u001b[32m     79\u001b[39m     \u001b[38;5;28mself\u001b[39m, \n\u001b[32m     80\u001b[39m     x: torch.Tensor, \n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m     \u001b[32m10\u001b[39m, scale: \u001b[38;5;28mint\u001b[39m = \u001b[32m1000\u001b[39m\n\u001b[32m     84\u001b[39m ) -> torch.Tensor:\n\u001b[32m     85\u001b[39m     x = torch.from_numpy(x)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     y = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m     x_sqnorms = torch.diag(torch.matmul(x, x.T))\n\u001b[32m     89\u001b[39m     y_sqnorms = torch.diag(torch.matmul(y, y.T))\n",
      "\u001b[31mTypeError\u001b[39m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "iteration = 4\n",
    "ref_data_path = 'data/celeba/female_test'\n",
    "\n",
    "for exp_name in ['dim_128_aplha_0.005_27.01.25_21:56:36']:#, 'dim_128_aplha_0.01_14.01.25_21:22:30']: \n",
    "    cmmd = CMMD().to(device)\n",
    "    gen_data_path = f'experiments/quantized_images/celeba/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        SomeDataset(data_dir=ref_data_path), batch_size=32\n",
    "    )\n",
    "    for real_images in tqdm(dataloader):\n",
    "        real_images = real_images.to(device)\n",
    "        cmmd.update(real_images, real=True)\n",
    "\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        SomeDataset(data_dir=gen_data_path), batch_size=32\n",
    "    )\n",
    "    for fake_images in tqdm(dataloader):\n",
    "        fake_images = fake_images.to(device)\n",
    "        cmmd.update(fake_images, real=False)\n",
    "    \n",
    "    print(f'Iter: {iteration}, CMMD: {cmmd.compute().detach().cpu().numpy()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaaf787",
   "metadata": {},
   "source": [
    "## MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55dbab6-87ed-43c3-84e1-f1525cc22e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for exp_name in ['small_dim_128_aplha_0.01_20.01.25_16:43:26']:\n",
    "    for iteration in range(2, 7):\n",
    "        gen_data_path = f'../experiments/quantized_images/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "        mse = calculate_mse(\n",
    "            eval_dir=gen_data_path,\n",
    "            ref_dir='../data/celeba/',\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        print(f'Iter: {iteration}, : {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
