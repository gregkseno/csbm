{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "729e75c8-837b-463b-b4e6-d3fc71f4b7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys \n",
    "sys.path.append('../')\n",
    "sys.path.append('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6f4a406-3e81-4519-8325-dd89ee4e77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "from typing import Literal, Optional\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorch_fid.fid_score import compute_statistics_of_path, calculate_frechet_distance, calculate_fid_given_paths\n",
    "from pytorch_fid.inception import InceptionV3\n",
    "from transformers import CLIPImageProcessor, CLIPVisionModelWithProjection\n",
    "\n",
    "from scripts.eval import calculate_fid, calculate_cmmd, calculate_mse, compute_embeddings_for_dir, ClipEmbeddingModel\n",
    "from csbm.models.quantized_images import Codec\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34209136-d23b-40e3-a1b1-97ae82f84c6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CelebaDataset:\n",
    "    transform: Optional[transforms.Compose] = None\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        sex: Literal['male', 'female'], \n",
    "        data_dir: str,\n",
    "        size: Optional[int] = None, \n",
    "        train: bool = True,\n",
    "        return_names: bool = False,\n",
    "        count: int = 0\n",
    "    ):\n",
    "        self.train = train\n",
    "        self.size = size\n",
    "        self.return_names = return_names\n",
    "\n",
    "        attrs = pd.read_csv(os.path.join(data_dir, 'celeba', 'list_attr_celeba.csv'))\n",
    "        if sex == 'male':\n",
    "            attrs = attrs[attrs['Male'] != -1] # only males\n",
    "        else:\n",
    "            attrs = attrs[attrs['Male'] == -1]\n",
    "        image_names = attrs['image_id'].tolist()\n",
    "        self.dataset = [os.path.join(data_dir, 'celeba', 'img_align_celeba', 'raw', image) for image in image_names][-count:]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # transform = transforms.Compose([\n",
    "        #     transforms.Resize((self.size, self.size)),\n",
    "        #     transforms.ToTensor(),\n",
    "        # ])\n",
    "        image = Image.open(self.dataset[index])\n",
    "        image = image.convert('RGB')\n",
    "        # image = transform(image)\n",
    "\n",
    "        if self.return_names:\n",
    "           return image, self.dataset[index].split('/')[-1]\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b49aee26-5272-493c-afe1-29b72873fcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_data_path = '../data/celeba/female_test'\n",
    "stats_ref_path = os.path.join(ref_data_path, 'fid_stats.npz')\n",
    "embd_ref_path = os.path.join(ref_data_path, 'cmmd_embed.npy')\n",
    "\n",
    "# gen_data_path = '../experiments/quantized_images/uniform/dim_128_aplha_0.01_14.01.25_21:22:30/checkpoints/forward_10/generation'\n",
    "gen_data_path = '../experiments/quantized_images/uniform/small_dim_128_aplha_0.01_20.01.25_16:43:26/checkpoints/forward_4/generation'\n",
    "\n",
    "dims = 2048\n",
    "batch_size = 32\n",
    "num_workers = 1\n",
    "max_count = int(0.1 * len(CelebaDataset('female', '../data/'))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c338ebc0-bf7e-4f58-a877-ba5de9316890",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(stats_ref_path):\n",
    "    stats_ref = np.load(stats_ref_path)\n",
    "else:\n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "    model = InceptionV3([block_idx]).to(device)\n",
    "    m, s = compute_statistics_of_path(\n",
    "        os.path.join(ref_data_path),\n",
    "        model, batch_size, dims, device, num_workers, max_count\n",
    "    )\n",
    "    stats_ref = {'mu': m, 'sigma': s}\n",
    "    np.savez(stats_ref_path, mu=m, sigma=s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70d3008b-5b80-4e5b-8722-a4eb2a6ba0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(embd_ref_path):\n",
    "    embs_ref = np.load(embd_ref_path).astype(\"float32\")\n",
    "else:\n",
    "    embs_ref = compute_embeddings_for_dir(\n",
    "        os.path.join(ref_data_path),\n",
    "        ClipEmbeddingModel(), batch_size, max_count\n",
    "    ).astype(\"float32\")\n",
    "    np.save(embd_ref_path, embs_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b385277-c54c-44a1-ba98-2e24bfe97eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 370/370 [00:44<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3, FID: 8.245995455934349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 370/370 [00:30<00:00, 11.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4, FID: 10.600888926217465\n"
     ]
    }
   ],
   "source": [
    "for exp_name in ['dim_128_aplha_0.005_27.01.25_21:56:36']: # ['dim_128_aplha_0.01_14.01.25_21:22:30']: #, 'small_dim_128_aplha_0.01_20.01.25_16:43:26']: #'tiny_dim_128_aplha_0.01_17.01.25_22:02:58', 'tiny_dim_128_aplha_0.01_19.01.25_21:21:21']:\n",
    "    for iteration in range(4, 5):\n",
    "        gen_data_path = f'../experiments/quantized_images/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "        fid = calculate_fid(\n",
    "            eval_dir=gen_data_path,\n",
    "            stats_ref=stats_ref,\n",
    "            dims=dims,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            device=device,\n",
    "            max_count=max_count\n",
    "        )\n",
    "        print(f'Iter: {iteration}, FID: {fid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8580d5-bb77-4640-8ed5-e71cec460ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▌                                                                                                   | 13/370 [00:01<00:32, 11.15it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m      3\u001b[0m     gen_data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../experiments/quantized_images/uniform/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/checkpoints/forward_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/generation\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_fid\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_ref\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_count\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIter: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, FID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfid\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/beegfs/home/g.ksenofontov/Projects/csbm/notebooks/../scripts/eval.py:259\u001b[0m, in \u001b[0;36mcalculate_fid\u001b[0;34m(eval_dir, stats_ref, dims, batch_size, num_workers, max_count, device)\u001b[0m\n\u001b[1;32m    256\u001b[0m model \u001b[38;5;241m=\u001b[39m InceptionV3([block_idx])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    258\u001b[0m mu_ref, sigma_ref \u001b[38;5;241m=\u001b[39m stats_ref[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m], stats_ref[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigma\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 259\u001b[0m mu_gen, sigma_gen \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_statistics_of_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_count\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m calculate_frechet_distance(mu1\u001b[38;5;241m=\u001b[39mmu_gen, sigma1\u001b[38;5;241m=\u001b[39msigma_gen, mu2\u001b[38;5;241m=\u001b[39mmu_ref, sigma2\u001b[38;5;241m=\u001b[39msigma_ref)\n",
      "File \u001b[0;32m~/anaconda3/envs/csbm/lib/python3.12/site-packages/pytorch_fid/fid_score.py:243\u001b[0m, in \u001b[0;36mcompute_statistics_of_path\u001b[0;34m(path, model, batch_size, dims, device, num_workers, max_count)\u001b[0m\n\u001b[1;32m    240\u001b[0m     path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path)\n\u001b[1;32m    241\u001b[0m     files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([file \u001b[38;5;28;01mfor\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m IMAGE_EXTENSIONS\n\u001b[1;32m    242\u001b[0m                    \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m path\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*.\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(ext))][:max_count])\n\u001b[0;32m--> 243\u001b[0m     m, s \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_activation_statistics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m m, s\n",
      "File \u001b[0;32m~/anaconda3/envs/csbm/lib/python3.12/site-packages/pytorch_fid/fid_score.py:228\u001b[0m, in \u001b[0;36mcalculate_activation_statistics\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_activation_statistics\u001b[39m(files, model, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[1;32m    210\u001b[0m                                     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    211\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Calculation of the statistics used by the FID.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    Params:\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    -- files       : List of image files paths\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m               the inception model.\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 228\u001b[0m     act \u001b[38;5;241m=\u001b[39m \u001b[43mget_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     mu \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(act, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    230\u001b[0m     sigma \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(act, rowvar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/csbm/lib/python3.12/site-packages/pytorch_fid/fid_score.py:143\u001b[0m, in \u001b[0;36mget_activations\u001b[0;34m(files, model, batch_size, dims, device, num_workers)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m pred\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    141\u001b[0m     pred \u001b[38;5;241m=\u001b[39m adaptive_avg_pool2d(pred, output_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 143\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    145\u001b[0m pred_arr[start_idx:start_idx \u001b[38;5;241m+\u001b[39m pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m pred\n\u001b[1;32m    147\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m start_idx \u001b[38;5;241m+\u001b[39m pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for exp_name in ['dim_128_aplha_0.005_27.01.25_21:56:36']: \n",
    "    fid = FrechetInceptionDistance(feature=2048)\n",
    "    for iteration in range(3, 5):\n",
    "        gen_data_path = f'../experiments/quantized_images/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "        fid = calculate_fid(\n",
    "            eval_dir=gen_data_path,\n",
    "            stats_ref=stats_ref,\n",
    "            dims=dims,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers,\n",
    "            device=device,\n",
    "            max_count=max_count\n",
    "        )\n",
    "        print(f'Iter: {iteration}, FID: {fid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e876bd76-95c9-425c-ab8f-24770486d2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating embeddings for 9344 images from ../experiments/quantized_images/uniform/dim_128_aplha_0.005_27.01.25_21:56:36/checkpoints/forward_3/generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2e93d4073f94c9d8752b20c8283f40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMMD: 0.15425682067871094\n",
      "Calculating embeddings for 11816 images from ../experiments/quantized_images/uniform/dim_128_aplha_0.005_27.01.25_21:56:36/checkpoints/forward_4/generation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a8b86a70fb745469c28b6b8a68fab59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/369 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMMD: 0.16450881958007812\n"
     ]
    }
   ],
   "source": [
    "for exp_name in ['dim_128_aplha_0.005_27.01.25_21:56:36']: # ['dim_128_aplha_0.01_14.01.25_21:22:30']: #, 'small_dim_128_aplha_0.01_20.01.25_16:43:26']: #'tiny_dim_128_aplha_0.01_17.01.25_22:02:58', 'tiny_dim_128_aplha_0.01_19.01.25_21:21:21']:\n",
    "    for iteration in range(4, 5):\n",
    "        gen_data_path = f'../experiments/quantized_images/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "        cmmd = calculate_cmmd(\n",
    "            eval_dir=gen_data_path,\n",
    "            embs_ref=embs_ref,\n",
    "            batch_size=batch_size,\n",
    "            max_count=max_count\n",
    "        )\n",
    "        print(f'CMMD: {cmmd}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f55dbab6-87ed-43c3-84e1-f1525cc22e6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b607b4495c09479c9088516bef0166db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2, : 0.029523000946706575\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b49ac5311924cdd8b947004f1664c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 3, : 0.01858686597710519\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19aec35acb994cf59238aeac33f2a8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x155467428a40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/trinity/home/g.ksenofontov/anaconda3/envs/disc_sbm/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/trinity/home/g.ksenofontov/anaconda3/envs/disc_sbm/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/trinity/home/g.ksenofontov/anaconda3/envs/disc_sbm/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 4, : 0.014704428473130447\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97dc078fc864b459004142badd6118d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 5, : 0.012771712402777892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1a19a5c429452bbf010f92998ca9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<function _MultiProcessingDataLoaderIter.__del__ at 0x155467428a40>\n",
      "Traceback (most recent call last):\n",
      "  File \"/trinity/home/g.ksenofontov/anaconda3/envs/disc_sbm/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1477, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/trinity/home/g.ksenofontov/anaconda3/envs/disc_sbm/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1460, in _shutdown_workers\n",
      "    if w.is_alive():\n",
      "       ^^^^^^^^^^^^\n",
      "  File \"/trinity/home/g.ksenofontov/anaconda3/envs/disc_sbm/lib/python3.12/multiprocessing/process.py\", line 160, in is_alive\n",
      "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: can only test a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 6, : 0.0114071473567727\n"
     ]
    }
   ],
   "source": [
    "for exp_name in ['small_dim_128_aplha_0.01_20.01.25_16:43:26']:\n",
    "    for iteration in range(2, 7):\n",
    "        gen_data_path = f'../experiments/quantized_images/uniform/{exp_name}/checkpoints/forward_{iteration}/generation'\n",
    "        mse = calculate_mse(\n",
    "            eval_dir=gen_data_path,\n",
    "            ref_dir='../data/celeba/',\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "        print(f'Iter: {iteration}, : {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7423e12f-694d-4f5a-810f-b8fd37b36849",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_quantizer_config = {\n",
    "    'config_path': '../configs/vqgan_celeba_f8_1024.yaml',\n",
    "    'ckpt_path': '../checkpoints/vqgan_celeba_f8_1024.ckpt',\n",
    "}\n",
    "vq = Codec(**vector_quantizer_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
